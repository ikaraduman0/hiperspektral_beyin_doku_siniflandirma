{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Hyperspectral Imageları kalibre ederek bant boyutunu azaltma ve hasta klasörlerine kayıt etme *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 8948,
     "status": "ok",
     "timestamp": 1734550467948,
     "user": {
      "displayName": "asist",
      "userId": "11091362440086264928"
     },
     "user_tz": -180
    },
    "id": "c_xBell98RBs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import spectral.io.envi as envi\n",
    "from scipy.ndimage import uniform_filter, gaussian_filter\n",
    "from sklearn.decomposition import PCA #,FastICA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU'nun kullanılabilirliğini kontrol et ve aktif yap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow versiyonu: 2.10.1\n",
      "Kullanılabilir GPU sayısı:  1\n",
      "Kullanılabilir GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Dinamik Bellek Kullanımı Ayarlandı!\n"
     ]
    }
   ],
   "source": [
    "# tensorflow versiyon kontrolü\n",
    "print(\"TensorFlow versiyonu:\", tf.__version__)\n",
    "\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Kullanılabilir GPU sayısı: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if gpu_devices:\n",
    "    print(\"Kullanılabilir GPU:\", gpu_devices)\n",
    "    tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')\n",
    "    try:\n",
    "        # Dinamik bellek tahsisi ayarla\n",
    "        tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "        print(\"Dinamik Bellek Kullanımı Ayarlandı!\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"GPU bulunamadı\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalibrasyon ve filtreleme fonksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1734550472719,
     "user": {
      "displayName": "asist",
      "userId": "11091362440086264928"
     },
     "user_tz": -180
    },
    "id": "p3wOBFtZ8eJ0"
   },
   "outputs": [],
   "source": [
    "# Kalibrasyon fonksiyonu\n",
    "def calibrate_hyperspectral_data(raw, dark_ref, white_ref):\n",
    "    return (raw - dark_ref) / (white_ref - dark_ref)\n",
    "\n",
    "def noise(data, filter_size=5):\n",
    "    # Uzaysal filtreleme: her bant için uygulanır    \n",
    "    return uniform_filter(data, size=filter_size)\n",
    "    \"\"\"\n",
    "    filtered_data = np.empty_like(data)\n",
    "    for i in range(data.shape[2]):  # Bantlar boyunca dön\n",
    "        filtered_data[:, :, i] = uniform_filter(data[:, :, i], size=filter_size)\n",
    "    return filtered_data\n",
    "    \"\"\"\n",
    "\n",
    "# Spektral Düzeltme (Gaussian Smoothing)\n",
    "def gauss(data, sigma=1):    \n",
    "    return gaussian_filter(data, sigma=(0, 0, sigma))\n",
    "\n",
    "# Normalize Etme\n",
    "def normalize(data):    \n",
    "    \"\"\"filtered_data = np.empty_like(data)\n",
    "    for i in range(data.shape[2]):  # Bantlar boyunca dön\n",
    "        min_val = np.min(data[:, :, i])\n",
    "        max_val = np.max(data[:, :, i])\n",
    "        filtered_data[:, :, i] = (data[:, :, i] - min_val) / (data[:, :, i] - min_val+ 1e-9)    \n",
    "    return filtered_data\n",
    "    \"\"\"\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    return (data - min_val) / (max_val - min_val+ 1e-9)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image gruplama foksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_spectral_bands(image, target_depth):\n",
    "    \"\"\"\n",
    "    Hiperspektral görüntüyü PCA kullanarak hedef bant sayısına indirger.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Hiperspektral görüntü (row, col, bands).\n",
    "        target_depth (int): Hedef spektral bant sayısı.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: PCA ile indirgenmiş spektral bantlar.\n",
    "    \"\"\"\n",
    "    # Orijinal boyutlar\n",
    "    rows, cols, bands = image.shape\n",
    "    image_reshaped = image.reshape(-1, bands)  # (row*col, bands)\n",
    "\n",
    "    # PCA ile indirgeme\n",
    "    pca = PCA(n_components=target_depth)\n",
    "    reduced_data = pca.fit_transform(image_reshaped)  # (row*col, target_depth)\n",
    "\n",
    "    # Yeniden şekillendirme\n",
    "    reduced_image = reduced_data.reshape(rows, cols, target_depth)\n",
    "    return reduced_image\n",
    "    \n",
    "def pcagrupla(hyperspectral_image, wavelength, bandgrup=5):\n",
    "    \"\"\"\n",
    "    HSI verisini PCA ile bantları birleştirerek azaltır ve yeni bir wavelength listesi döner.\n",
    "\n",
    "    Args:\n",
    "    - hyperspectral_image: Hyperspectral görüntü (yükseklik, genişlik, bant sayısı)\n",
    "    - wavelength: Orijinal wavelength değerlerinin listesi\n",
    "    - bandgrup: Birleştirilecek bant sayısı\n",
    "\n",
    "    Returns:\n",
    "    - new_image: PCA ile indirgenmiş yeni görüntü\n",
    "    - new_wavelength: Yeni wavelength listesi (her grubun ilk ve son dalga boyları birleştirilmiş)\n",
    "    \"\"\"\n",
    "    height, width, num_bands = hyperspectral_image.shape\n",
    "\n",
    "    # Her bandgrup bandı birleştirip PCA ile 1'e düşür\n",
    "    bands_to_combine = bandgrup\n",
    "    new_num_bands = num_bands // bands_to_combine  # Yeni bant sayısı\n",
    "    new_image = np.zeros((height, width, new_num_bands))\n",
    "    new_wavelength = []\n",
    "\n",
    "    for i in range(new_num_bands):\n",
    "        # Bant aralığını al\n",
    "        start_band = i * bands_to_combine\n",
    "        end_band = start_band + bands_to_combine\n",
    "        image = hyperspectral_image[:, :, start_band:end_band]\n",
    "\n",
    "        # Wave length aralığını birleştir (ilk ve son değer)\n",
    "        group_wavelength = f\"{wavelength[start_band]:.2f}-{wavelength[end_band-1]:.2f}\"\n",
    "        new_wavelength.append(group_wavelength)\n",
    "\n",
    "        # PCA uygulama\n",
    "        image_reshaped = image.reshape(-1, bandgrup)  # (row*col, bands)\n",
    "        pca = PCA(n_components=1)\n",
    "        reduced_data = pca.fit_transform(image_reshaped)  # (row*col, target_depth)\n",
    "        \n",
    "        # İndirgenmiş veriyi yeni banda ekle\n",
    "        new_image[:, :, i] = reduced_data.reshape(height, width)\n",
    "\n",
    "    return new_image, new_wavelength    \n",
    "\n",
    "\n",
    "    \n",
    "def parcagrupla(hyperspectral_image,wavelength, bandgrup=5):\n",
    "    \"\"\"\n",
    "    HSI verisini belirtilen bant grupta bir bant alır. Yeni bir image ve wavelength listesi döner.\n",
    "\n",
    "    Args:\n",
    "    - hyperspectral_image: Hyperspectral görüntü (yükseklik, genişlik, bant sayısı)\n",
    "    - wavelength: Orijinal wavelength değerlerinin listesi\n",
    "    - bandgrup: Birleştirilecek bant sayısı\n",
    "\n",
    "    Returns:\n",
    "    - new_image: Her bantgrupla  alınan ve indirgenmiş yeni görüntü\n",
    "    - new_wavelength: Yeni wavelength listesi (her grubun ilk ve son dalga boyları birleştirilmiş)\n",
    "    \"\"\"  \n",
    "    \n",
    "  height, width, num_bands = hyperspectral_image.shape\n",
    "\n",
    "  # Her bandgrup bandı birleştiripaliıp birlestirerek yeni bir görüntü oluştur\n",
    "  bands_to_combine = bandgrup\n",
    "  new_num_bands = num_bands // bands_to_combine  # Yeni bant sayısı\n",
    "  new_image = np.zeros((height, width, new_num_bands))\n",
    "  new_wavelength = []\n",
    "\n",
    "  for i in range(new_num_bands):\n",
    "      # Bant aralığını al\n",
    "      start_band = i * bands_to_combine      \n",
    "\n",
    "      new_wavelength.append(f\"{wavelength[start_band]:.2f}\")\n",
    "\n",
    "      # yeni banda ekle\n",
    "      new_image[:, :, i] = hyperspectral_image[:, :, start_band]\n",
    "\n",
    "  \n",
    "  return new_image,new_wavelength\n",
    "\n",
    "def ortalamagrupla(hyperspectral_image,wavelength,bandgrup=5):\n",
    "    \"\"\"\n",
    "    HSI verisini her bantgrup imageyi Ortalama ile bantları birleştirerek azaltır ve yeni bir wavelength listesi döner.\n",
    "\n",
    "    Args:\n",
    "    - hyperspectral_image: Hyperspectral görüntü (yükseklik, genişlik, bant sayısı)\n",
    "    - wavelength: Orijinal wavelength değerlerinin listesi\n",
    "    - bandgrup: Birleştirilecek bant sayısı\n",
    "\n",
    "    Returns:\n",
    "    - new_image: ORtalama ile indirgenmiş yeni görüntü\n",
    "    - new_wavelength: Yeni wavelength listesi (her grubun ilk ve son dalga boyları birleştirilmiş)\n",
    "    \"\"\"\n",
    "  # Örnek hyperspectral görüntü (boyut: [height, width, num_bands])\n",
    "  # Kendi verinizi buraya yükleyin\n",
    "  height, width, num_bands = hyperspectral_image.shape\n",
    "\n",
    "  # Her bandgrup bandı birleştirip ortalamasını alarak yeni bir görüntü oluştur\n",
    "  bands_to_combine = bandgrup\n",
    "  new_num_bands = num_bands // bands_to_combine  # Yeni bant sayısı\n",
    "  new_image = np.zeros((height, width, new_num_bands))\n",
    "  new_wavelength = []\n",
    "    \n",
    "  for i in range(new_num_bands):\n",
    "      # Bant aralığını al\n",
    "      start_band = i * bands_to_combine\n",
    "      end_band = start_band + bands_to_combine\n",
    "\n",
    "      # Wave length aralığını birleştir (ilk ve son değer)\n",
    "      group_wavelength = f\"{wavelength[start_band]:.2f}-{wavelength[end_band-1]:.2f}\"\n",
    "      new_wavelength.append(group_wavelength)      \n",
    "\n",
    "      # Ortalama alarak yeni banda ekle\n",
    "      new_image[:, :, i] = np.mean(hyperspectral_image[:, :, start_band:end_band], axis=2)\n",
    "\n",
    "  \n",
    "  return new_image,new_wavelength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image işleme fonksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1734550799039,
     "user": {
      "displayName": "asist",
      "userId": "11091362440086264928"
     },
     "user_tz": -180
    },
    "id": "JmxtnVA78g1h"
   },
   "outputs": [],
   "source": [
    "def imgsave(destination_folder,filename,image,waves,metadata={}):\n",
    "    # Image yi belirtilen hedefe kayıt eder\n",
    "    \n",
    "    # Çıktı dosyası adı\n",
    "    hdr = os.path.join(destination_folder, filename)\n",
    "    \n",
    "    metadata['wavegroup'] = waves\n",
    "    \n",
    "    # Kalibre edilmiş veriyi kaydet\n",
    "    envi.save_image(\n",
    "        hdr,\n",
    "        image,\n",
    "        metadata=metadata,\n",
    "        dtype=np.float32,\n",
    "        interleave=\"bil\",  # Band Interleave\n",
    "        force=True  # Üzerine yazmayı etkinleştir\n",
    "    )          \n",
    "\n",
    "# Ana işlem fonksiyonu\n",
    "def process_dataset(dataset_path,destination_path,parca=5):     \n",
    "    \n",
    "    # Belirtilen yolda Tüm alt klasörleri gez\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        # Kalibrasyon için gerekli tüm dosyalar var ise işleme geç\n",
    "        if \"darkReference.hdr\" in files and \"whiteReference.hdr\" in files and \"raw.hdr\" in files:\n",
    "            print(f\"Processing folder: {root}\")\n",
    "\n",
    "            # dataset_path'i çıkararak göreli yolu (Hasta klasörü adını) al\n",
    "            relative_path = os.path.basename(root)\n",
    "\n",
    "            # Hedef yolda göreli yolu birleştir\n",
    "            destination_folder = os.path.join(destination_path, relative_path)\n",
    "\n",
    "            # Hedef klasör yoksa oluştur\n",
    "            if not os.path.exists(destination_folder):\n",
    "                os.makedirs(destination_folder)\n",
    "                print(f\"Hedef klasör oluşturuldu: {destination_folder}\")\n",
    "\n",
    "            # Dosya yollarını oluştur\n",
    "            dark_hdr = os.path.join(root, \"darkReference.hdr\")\n",
    "            white_hdr = os.path.join(root, \"whiteReference.hdr\")\n",
    "            raw_hdr = os.path.join(root, \"raw.hdr\")\n",
    "\n",
    "            # Verinin bulunduğu uzantısız dosyaları belirle\n",
    "            dark_data_path = dark_hdr.replace(\".hdr\", \"\")  # Uzantısız veri dosyası\n",
    "            white_data_path = white_hdr.replace(\".hdr\", \"\")  # Uzantısız veri dosyası\n",
    "            raw_data_path = raw_hdr.replace(\".hdr\", \"\")  # Uzantısız veri dosyası\n",
    "\n",
    "            # gtMap dosyalarını hedef klasöre kopyala\n",
    "            # 0 dan olusturulur iken burasını açıp çalıştır\n",
    "            \n",
    "            # gtmap_hdr_source=os.path.join(root, \"gtMap.hdr\")\n",
    "            # gtmap_hdr_destination = os.path.join(destination_folder, \"gtMap.hdr\")\n",
    "            # shutil.copy(gtmap_hdr_source, gtmap_hdr_destination)\n",
    "            # gtmap_hdr_destination=gtmap_hdr_destination.replace(\".hdr\", \"\")\n",
    "            # print(f\"gtMap.hdr kopyalandı : {gtmap_hdr_destination}\")\n",
    "            # shutil.copy(gtmap_data_path, gtmap_hdr_destination)\n",
    "            # print(f\"gtMap kopyalandı: {gtmap_hdr_destination}\")\n",
    "            \n",
    "            # ENVI formatındaki verileri yükle\n",
    "            dark_ref = envi.open(dark_hdr, dark_data_path).load()\n",
    "            white_ref = envi.open(white_hdr, white_data_path).load()\n",
    "            raw_data = envi.open(raw_hdr, raw_data_path).load()\n",
    "\n",
    "            # Kalibrasyon işlemi\n",
    "            calibrated_data = calibrate_hyperspectral_data(raw_data, dark_ref, white_ref)\n",
    "\n",
    "            # Aşırı Bant Gürültüsü Giderme (Extreme Band Noise Removal) ilk 56 son 126 makaleden alınan bilgi\n",
    "            calibrated_data = calibrated_data[:, :, 56:-126]\n",
    "\n",
    "            # Kırpılmış wavelength bilgilerini al            \n",
    "            metadata = raw_data.metadata            \n",
    "            original_wavelengths = np.array(metadata['wavelength'], dtype=float)\n",
    "            \n",
    "            #RGB image olusturmak icin belirtilen bant noları\n",
    "            default_bands=np.array(metadata['default bands'], dtype=int)\n",
    "            \n",
    "            #calibrated_data ya kayıt sırasında eşleşen dalgaboyu degerlerini almak icin aynı kırpma islemi uggula\n",
    "            cropped_wavelengths = original_wavelengths[56:-126].tolist()    \n",
    "\n",
    "            # Metadata'yı güncelle\n",
    "            metadata={}           \n",
    "            # calibrated_data daki kırpmalardan dolayı RGB oluşturacak bant degerleri de aynı oranda kırpılmalı\n",
    "            metadata['default bands'] =  [band - 56 for band in default_bands]\n",
    "\n",
    "            imgsave(destination_folder,filename=f\"calibrated_raw.hdr\",image=calibrated_data,waves=cropped_wavelengths,metadata=metadata)\n",
    "\n",
    "            # Filteleme işlemleri eğitim sırasında yapılacak\n",
    "            \n",
    "            # Gürültü Filtreleme\n",
    "            # calibrated_data = noise_filtering(calibrated_data, filter_size=5)\n",
    "\n",
    "            # Normalizasyon\n",
    "            # calibrated_data = normalize_data(calibrated_data)\n",
    "\n",
    "            # PCA belirtilen her parcalık bandı PCA islemi yaparak tek banda düşür ve ardından tüm PCA lı bantları birleştir ve yeni Image dosyasını kaydet\n",
    "            \n",
    "            pcaimage,pcawave=pcagrupla(calibrated_data,cropped_wavelengths,bandgrup=parca)\n",
    "            pcawave=cropped_wavelengths            \n",
    "            \n",
    "            imgsave(destination_folder,filename=f\"calibrated_pca_{parca}.hdr\",image=pcaimage,waves=pcawave)\n",
    "\n",
    "            # ORTALAMA belirtilen her parcalık bandı Ortalamasını yaparak tek banda düşür ve ardından tüm Ortalaması alınmış bantları birleştir ve yeni Image dosyasını kaydet\n",
    "            \n",
    "            ortimage,ortwave=ortalamagrupla(calibrated_data,cropped_wavelengths,bandgrup=parca)\n",
    "            imgsave(destination_folder,filename=f\"calibrated_ort_{parca}.hdr\",image=ortimage,waves=ortwave)\n",
    "\n",
    "            # PARCA belirtilen her parcadaki 1 bandı al sonrasında alınmış bantları birleştir ve yeni Image dosyasını kaydet\n",
    "            \n",
    "            parcaimage,parcawave=parcagrupla(calibrated_data,cropped_wavelengths,bandgrup=parca)            \n",
    "            imgsave(destination_folder,filename=f\"calibrated_par_{parca}.hdr\",image=parcaimage,waves=parcawave)\n",
    "\n",
    "\n",
    "            print(f\"Calibrated data: {calibrated_data.shape} pca : {pcaimage.shape}  ort: {ortimage.shape} par: {parcaimage.shape} \\n {destination_folder}\")\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image ları işle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1688661,
     "status": "ok",
     "timestamp": 1734552493579,
     "user": {
      "displayName": "asist",
      "userId": "11091362440086264928"
     },
     "user_tz": -180
    },
    "id": "XmHCa-268ol3",
    "outputId": "c5f1fbe6-373d-473c-a935-cecfd365b205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hedef klasör yolu mevcut : D:/proje/Data/hasta\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\004-02\\004-02\n",
      "Calibrated data: (389, 345, 644) pca : (389, 345, 128)  \n",
      " D:/proje/Data/hasta\\004-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\005-01\\005-01\n",
      "Calibrated data: (483, 488, 644) pca : (483, 488, 128)  \n",
      " D:/proje/Data/hasta\\005-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\007-01\\007-01\n",
      "Calibrated data: (582, 400, 644) pca : (582, 400, 128)  \n",
      " D:/proje/Data/hasta\\007-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\008-01\\008-01\n",
      "Calibrated data: (460, 549, 644) pca : (460, 549, 128)  \n",
      " D:/proje/Data/hasta\\008-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\008-02\\008-02\n",
      "Calibrated data: (480, 553, 644) pca : (480, 553, 128)  \n",
      " D:/proje/Data/hasta\\008-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\010-03\\010-03\n",
      "Calibrated data: (371, 461, 644) pca : (371, 461, 128)  \n",
      " D:/proje/Data/hasta\\010-03\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\012-01\\012-01\n",
      "Calibrated data: (443, 497, 644) pca : (443, 497, 128)  \n",
      " D:/proje/Data/hasta\\012-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\012-02\\012-02\n",
      "Calibrated data: (445, 498, 644) pca : (445, 498, 128)  \n",
      " D:/proje/Data/hasta\\012-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\013-01\\013-01\n",
      "Calibrated data: (298, 253, 644) pca : (298, 253, 128)  \n",
      " D:/proje/Data/hasta\\013-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\014-01\\014-01\n",
      "Calibrated data: (317, 244, 644) pca : (317, 244, 128)  \n",
      " D:/proje/Data/hasta\\014-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\015-01\\015-01\n",
      "Calibrated data: (376, 494, 644) pca : (376, 494, 128)  \n",
      " D:/proje/Data/hasta\\015-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\016-01\\016-01\n",
      "Calibrated data: (335, 323, 644) pca : (335, 323, 128)  \n",
      " D:/proje/Data/hasta\\016-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\016-02\\016-02\n",
      "Calibrated data: (335, 326, 644) pca : (335, 326, 128)  \n",
      " D:/proje/Data/hasta\\016-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\016-03\\016-03\n",
      "Calibrated data: (315, 321, 644) pca : (315, 321, 128)  \n",
      " D:/proje/Data/hasta\\016-03\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\016-04\\016-04\n",
      "Calibrated data: (383, 297, 644) pca : (383, 297, 128)  \n",
      " D:/proje/Data/hasta\\016-04\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\016-05\\016-05\n",
      "Calibrated data: (414, 292, 644) pca : (414, 292, 128)  \n",
      " D:/proje/Data/hasta\\016-05\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\017-01\\017-01\n",
      "Calibrated data: (441, 399, 644) pca : (441, 399, 128)  \n",
      " D:/proje/Data/hasta\\017-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\018-01\\018-01\n",
      "Calibrated data: (479, 462, 644) pca : (479, 462, 128)  \n",
      " D:/proje/Data/hasta\\018-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\018-02\\018-02\n",
      "Calibrated data: (510, 434, 644) pca : (510, 434, 128)  \n",
      " D:/proje/Data/hasta\\018-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\019-01\\019-01\n",
      "Calibrated data: (601, 535, 644) pca : (601, 535, 128)  \n",
      " D:/proje/Data/hasta\\019-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\020-01\\020-01\n",
      "Calibrated data: (378, 330, 644) pca : (378, 330, 128)  \n",
      " D:/proje/Data/hasta\\020-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\021-01\\021-01\n",
      "Calibrated data: (452, 334, 644) pca : (452, 334, 128)  \n",
      " D:/proje/Data/hasta\\021-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\021-02\\021-02\n",
      "Calibrated data: (448, 324, 644) pca : (448, 324, 128)  \n",
      " D:/proje/Data/hasta\\021-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\021-05\\021-05\n",
      "Calibrated data: (433, 340, 644) pca : (433, 340, 128)  \n",
      " D:/proje/Data/hasta\\021-05\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\022-01\\022-01\n",
      "Calibrated data: (597, 527, 644) pca : (597, 527, 128)  \n",
      " D:/proje/Data/hasta\\022-01\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\022-02\\022-02\n",
      "Calibrated data: (611, 527, 644) pca : (611, 527, 128)  \n",
      " D:/proje/Data/hasta\\022-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\022-03\\022-03\n",
      "Calibrated data: (592, 471, 644) pca : (592, 471, 128)  \n",
      " D:/proje/Data/hasta\\022-03\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\025-02\\025-02\n",
      "Calibrated data: (473, 403, 644) pca : (473, 403, 128)  \n",
      " D:/proje/Data/hasta\\025-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\026-02\\026-02\n",
      "Calibrated data: (340, 324, 644) pca : (340, 324, 128)  \n",
      " D:/proje/Data/hasta\\026-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\027-02\\027-02\n",
      "Calibrated data: (493, 476, 644) pca : (493, 476, 128)  \n",
      " D:/proje/Data/hasta\\027-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\028-03\\028-03\n",
      "Calibrated data: (422, 398, 644) pca : (422, 398, 128)  \n",
      " D:/proje/Data/hasta\\028-03\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\028-04\\028-04\n",
      "Calibrated data: (482, 408, 644) pca : (482, 408, 128)  \n",
      " D:/proje/Data/hasta\\028-04\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\028-05\\028-05\n",
      "Calibrated data: (482, 390, 644) pca : (482, 390, 128)  \n",
      " D:/proje/Data/hasta\\028-05\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\029-02\\029-02\n",
      "Calibrated data: (365, 371, 644) pca : (365, 371, 128)  \n",
      " D:/proje/Data/hasta\\029-02\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\029-04\\029-04\n",
      "Calibrated data: (399, 342, 644) pca : (399, 342, 128)  \n",
      " D:/proje/Data/hasta\\029-04\n",
      "Processing folder: D:/proje/Data/npj_database\\FirstCampaign\\030-02\\030-02\n",
      "Calibrated data: (382, 285, 644) pca : (382, 285, 128)  \n",
      " D:/proje/Data/hasta\\030-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\034-01\\034-01\n",
      "Calibrated data: (319, 356, 644) pca : (319, 356, 128)  \n",
      " D:/proje/Data/hasta\\034-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\034-02\\034-02\n",
      "Calibrated data: (300, 342, 644) pca : (300, 342, 128)  \n",
      " D:/proje/Data/hasta\\034-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\034-03\\034-03\n",
      "Calibrated data: (290, 301, 644) pca : (290, 301, 128)  \n",
      " D:/proje/Data/hasta\\034-03\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\035-01\\035-01\n",
      "Calibrated data: (431, 503, 644) pca : (431, 503, 128)  \n",
      " D:/proje/Data/hasta\\035-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\035-02\\035-02\n",
      "Calibrated data: (312, 535, 644) pca : (312, 535, 128)  \n",
      " D:/proje/Data/hasta\\035-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\036-01\\036-01\n",
      "Calibrated data: (412, 324, 644) pca : (412, 324, 128)  \n",
      " D:/proje/Data/hasta\\036-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\036-02\\036-02\n",
      "Calibrated data: (432, 322, 644) pca : (432, 322, 128)  \n",
      " D:/proje/Data/hasta\\036-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\037-01\\037-01\n",
      "Calibrated data: (434, 453, 644) pca : (434, 453, 128)  \n",
      " D:/proje/Data/hasta\\037-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\037-02\\037-02\n",
      "Calibrated data: (315, 526, 644) pca : (315, 526, 128)  \n",
      " D:/proje/Data/hasta\\037-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\037-03\\037-03\n",
      "Calibrated data: (290, 422, 644) pca : (290, 422, 128)  \n",
      " D:/proje/Data/hasta\\037-03\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\037-04\\037-04\n",
      "Calibrated data: (280, 444, 644) pca : (280, 444, 128)  \n",
      " D:/proje/Data/hasta\\037-04\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\038-01\\038-01\n",
      "Calibrated data: (497, 490, 644) pca : (497, 490, 128)  \n",
      " D:/proje/Data/hasta\\038-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\039-01\\039-01\n",
      "Calibrated data: (415, 446, 644) pca : (415, 446, 128)  \n",
      " D:/proje/Data/hasta\\039-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\039-02\\039-02\n",
      "Calibrated data: (399, 439, 644) pca : (399, 439, 128)  \n",
      " D:/proje/Data/hasta\\039-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\040-01\\040-01\n",
      "Calibrated data: (303, 374, 644) pca : (303, 374, 128)  \n",
      " D:/proje/Data/hasta\\040-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\040-02\\040-02\n",
      "Calibrated data: (294, 344, 644) pca : (294, 344, 128)  \n",
      " D:/proje/Data/hasta\\040-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\041-01\\041-01\n",
      "Calibrated data: (449, 486, 644) pca : (449, 486, 128)  \n",
      " D:/proje/Data/hasta\\041-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\041-02\\041-02\n",
      "Calibrated data: (437, 488, 644) pca : (437, 488, 128)  \n",
      " D:/proje/Data/hasta\\041-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\042-01\\042-01\n",
      "Calibrated data: (629, 646, 644) pca : (629, 646, 128)  \n",
      " D:/proje/Data/hasta\\042-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\042-02\\042-02\n",
      "Calibrated data: (623, 584, 644) pca : (623, 584, 128)  \n",
      " D:/proje/Data/hasta\\042-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\042-03\\042-03\n",
      "Calibrated data: (650, 582, 644) pca : (650, 582, 128)  \n",
      " D:/proje/Data/hasta\\042-03\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\043-01\\043-01\n",
      "Calibrated data: (575, 543, 644) pca : (575, 543, 128)  \n",
      " D:/proje/Data/hasta\\043-01\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\043-02\\043-02\n",
      "Calibrated data: (554, 446, 644) pca : (554, 446, 128)  \n",
      " D:/proje/Data/hasta\\043-02\n",
      "Processing folder: D:/proje/Data/npj_database\\SecondCampaign\\043-04\\043-04\n",
      "Calibrated data: (538, 525, 644) pca : (538, 525, 128)  \n",
      " D:/proje/Data/hasta\\043-04\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\050-01\\050-01\n",
      "Calibrated data: (565, 533, 644) pca : (565, 533, 128)  \n",
      " D:/proje/Data/hasta\\050-01\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\051-01\\051-01\n",
      "Calibrated data: (635, 617, 644) pca : (635, 617, 128)  \n",
      " D:/proje/Data/hasta\\051-01\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\053-01\\053-01\n",
      "Calibrated data: (546, 446, 644) pca : (546, 446, 128)  \n",
      " D:/proje/Data/hasta\\053-01\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\054-01\\054-01\n",
      "Calibrated data: (515, 504, 644) pca : (515, 504, 128)  \n",
      " D:/proje/Data/hasta\\054-01\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\055-01\\055-01\n",
      "Calibrated data: (397, 435, 644) pca : (397, 435, 128)  \n",
      " D:/proje/Data/hasta\\055-01\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\055-02\\055-02\n",
      "Calibrated data: (500, 349, 644) pca : (500, 349, 128)  \n",
      " D:/proje/Data/hasta\\055-02\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\056-01\\056-01\n",
      "Calibrated data: (446, 598, 644) pca : (446, 598, 128)  \n",
      " D:/proje/Data/hasta\\056-01\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\056-02\\056-02\n",
      "Calibrated data: (467, 566, 644) pca : (467, 566, 128)  \n",
      " D:/proje/Data/hasta\\056-02\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\057-01\\057-01\n",
      "Calibrated data: (440, 535, 644) pca : (440, 535, 128)  \n",
      " D:/proje/Data/hasta\\057-01\n",
      "Processing folder: D:/proje/Data/npj_database\\ThirdCampaign\\058-02\\058-02\n",
      "Calibrated data: (721, 752, 644) pca : (721, 752, 128)  \n",
      " D:/proje/Data/hasta\\058-02\n"
     ]
    }
   ],
   "source": [
    "# Dataset yolu\n",
    "dataset_path = \"D:/proje/Data/npj_database\"\n",
    "destination_path = \"D:/proje/Data/hasta\"\n",
    "\n",
    "# Hedef yol yoksa oluştur\n",
    "if not os.path.exists(destination_path):\n",
    "    os.makedirs(destination_path)\n",
    "    print(f\"Hedef klasör yolu oluşturuldu : {destination_path}\")\n",
    "else:\n",
    "    print(f\"Hedef klasör yolu mevcut : {destination_path}\")\n",
    "\n",
    "# Dataseti işle\n",
    "process_dataset(dataset_path,destination_path,parca=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
